# -*- coding: utf-8 -*-
"""LLM Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-TJponzw9eq8re0doFVXmOO-Z_5_Uwk7
"""

from datasets import Dataset
import pandas as pd

# Load file
filepath = "Sentences_AllAgree.txt"

# Parse into text and label
texts, labels = [], []
with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
    for line in f:
        if "@positive" in line:
            label = "positive"
        elif "@negative" in line:
            label = "negative"
        elif "@neutral" in line:
            label = "neutral"
        else:
            continue  # skip malformed

        text = line.strip().split(f"@{label}")[0].strip()
        texts.append(text)
        labels.append(label)

# Create HF dataset
df = pd.DataFrame({"text": texts, "label": labels})
dataset = Dataset.from_pandas(df)

# Show sample
dataset[0]

def preprocess(example):
    example["input_text"] = f"sentiment analysis: {example['text']}"
    example["target_text"] = example["label"]
    return example

dataset = dataset.map(preprocess)
dataset = dataset.train_test_split(test_size=0.1)

dataset["train"][0]

from transformers import AutoTokenizer

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    input = tokenizer(example["input_text"], truncation=True, padding="max_length", max_length=128)
    label = tokenizer(example["target_text"], truncation=True, padding="max_length", max_length=8)

    input["labels"] = label["input_ids"]
    return input

tokenized = dataset.map(tokenize, remove_columns=dataset["train"].column_names)

from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_model, LoraConfig, TaskType

# Load base model
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define LoRA config
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none"
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Print trainable params
model.print_trainable_parameters()

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

# Data collator handles padding dynamically
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = Seq2SeqTrainingArguments(
    output_dir="./lora-finetuned-flan-t5-small",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_steps=50,
    save_steps=200,
    save_total_limit=2,
    num_train_epochs=3,
    learning_rate=5e-4,
    weight_decay=0.01,
    predict_with_generate=True,
    fp16=False,
    report_to="none"
)


trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

import evaluate
import torch

rouge = evaluate.load("rouge")

def generate_and_evaluate(orig_dataset, num_samples=10):
    model.eval()
    preds = []
    refs = []

    device = next(model.parameters()).device  # get model device (cuda or cpu)

    for example in orig_dataset.select(range(num_samples)):
        inputs = tokenizer(example["input_text"], return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}  # move inputs to model device

        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=10)

        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
        preds.append(pred)
        refs.append(example["target_text"])

        print(f"Input: {example['input_text']}")
        print(f"Prediction: {pred}")
        print(f"Reference: {example['target_text']}")
        print("-" * 50)

    result = rouge.compute(predictions=preds, references=refs)
    print("ROUGE scores:", result)


generate_and_evaluate(dataset["test"])

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

def classify_and_evaluate(orig_dataset, num_samples=100):
    model.eval()
    preds = []
    refs = []

    device = next(model.parameters()).device

    for example in orig_dataset.select(range(num_samples)):
        inputs = tokenizer(example["input_text"], return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=10)
        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()
        preds.append(pred)
        refs.append(example["target_text"].strip().lower())

    # Accuracy
    acc = accuracy_score(refs, preds)

    # Precision, Recall, F1 (macro averaged)
    precision, recall, f1, _ = precision_recall_fscore_support(refs, preds, average='macro')

    print(f"Accuracy: {acc:.4f}")
    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro): {recall:.4f}")
    print(f"F1 (macro): {f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(refs, preds))

classify_and_evaluate(dataset["test"])